# Transcribed 2023-11-10 with medium model size

1
0:00:00,000 --> 0:00:13,880
 The next speakers we have is Karen Melchior, a member of the European Parliament, and Jesper

2
0:00:13,880 --> 0:00:17,520
 Lund, the chair of the IT poll.

3
0:00:17,520 --> 0:00:21,000
 They will speak about chat control.

4
0:00:21,000 --> 0:00:23,480
 The next month will be critical.

5
0:00:23,480 --> 0:00:28,480
 Let's give them a hand.

6
0:00:28,480 --> 0:00:37,440
 Thank you very much and thanks for coming to this, I think, one of several presentations

7
0:00:37,440 --> 0:00:44,040
 about chat control, a surveillance proposal from the EU Commission.

8
0:00:44,040 --> 0:00:50,040
 And there was a talk yesterday by Jan and today we will focus on what is happening at

9
0:00:50,040 --> 0:00:58,000
 the political institutions in the EU and why the next couple of months will be critical

10
0:00:58,000 --> 0:01:02,760
 for fundamental rights in the years to come.

11
0:01:02,760 --> 0:01:05,360
 This is a quick outline.

12
0:01:05,360 --> 0:01:12,120
 We will do a very quick outline of what chat control is about and what is the real name

13
0:01:12,120 --> 0:01:14,280
 of chat control.

14
0:01:14,280 --> 0:01:18,800
 And then we will, without hopefully making it too boring, explain how the EU lawmaking

15
0:01:18,800 --> 0:01:27,840
 process works, because some understanding of that is very useful to, first of all, influence

16
0:01:27,880 --> 0:01:33,280
 the process, but also understand why does it matter that Council did this and that and

17
0:01:33,280 --> 0:01:35,760
 so forth.

18
0:01:35,760 --> 0:01:45,120
 And in the end we will suggest how you or others listening to this online or seeing

19
0:01:45,120 --> 0:01:52,960
 it afterwards can try to influence the process so that this mass surveillance proposal doesn't

20
0:01:52,960 --> 0:01:57,240
 become a reality.

21
0:01:57,640 --> 0:02:04,520
 And then this is really the crux of the issue when we're looking at the proposals.

22
0:02:04,520 --> 0:02:10,200
 Every time we have a presentation of the Commission's proposal, the proponents of the proposal will

23
0:02:10,200 --> 0:02:17,600
 start out with spending 10 minutes, 20 minutes out of an hour explaining how terrible sexual

24
0:02:17,600 --> 0:02:23,440
 abuse of children is and how it's grown online in the last couple of years and sort of really

25
0:02:23,640 --> 0:02:28,920
 explaining what has happened to the children and how difficult it is for the police to

26
0:02:28,920 --> 0:02:33,840
 prosecute the criminals that have committed this without actually going into sort of,

27
0:02:33,840 --> 0:02:36,920
 OK, there are criminal networks that are behind this and there are things going on in the

28
0:02:36,920 --> 0:02:38,200
 physical space.

29
0:02:38,200 --> 0:02:42,480
 They just jump to, OK, we need to do something about this online because we need to save

30
0:02:42,480 --> 0:02:43,800
 the children.

31
0:02:43,800 --> 0:02:47,000
 And look at these two cute young girls.

32
0:02:47,000 --> 0:02:50,620
 Who would be able to say no to save them from being abused?

33
0:02:50,620 --> 0:02:58,100
 Who would say no to actually help children from experiencing horrible things and being

34
0:02:58,100 --> 0:03:03,300
 subject to criminals and criminal networks and having the pictures and images shared

35
0:03:03,300 --> 0:03:05,100
 across the world?

36
0:03:05,100 --> 0:03:13,780
 That makes it very difficult to say the Children's Safety Online proposal, child sexual abuse

37
0:03:13,780 --> 0:03:19,600
 material handling is something that we don't want to do anything about.

38
0:03:19,600 --> 0:03:25,360
 And that is really the political environment and the discussion and the debate that we're

39
0:03:25,360 --> 0:03:27,880
 going into when we're talking about this.

40
0:03:27,880 --> 0:03:31,440
 So we need to remember to think about the feelings of the people that we're talking

41
0:03:31,440 --> 0:03:37,800
 to and not only think about the sort of technicalities because we all want to save children from

42
0:03:37,800 --> 0:03:42,920
 abuse but we actually want to do it in a way that works and what we have on the table doesn't

43
0:03:43,920 --> 0:03:51,680
 We want to let the politicians know that, I'm one of them, that we want to have legislation

44
0:03:51,680 --> 0:03:57,040
 that can save the kids in a way that works.

45
0:03:57,040 --> 0:04:02,480
 And as I was mentioning with the Kids' Online Safety Act, it's not only in the EU.

46
0:04:02,480 --> 0:04:06,160
 We are having proposals being made across the world.

47
0:04:06,160 --> 0:04:10,580
 We have in the US where they're also mixing it up with a little bit of some of the content

48
0:04:10,580 --> 0:04:13,500
 that we had in the DSA in the EU.

49
0:04:13,500 --> 0:04:21,020
 The UK also has an online safety bill that is proposing some of the same things on client

50
0:04:21,020 --> 0:04:22,020
 scanning.

51
0:04:22,020 --> 0:04:27,220
 So this is a sort of global phenomenon and therefore we need to help each other across

52
0:04:27,220 --> 0:04:32,540
 the different countries and regions to fight this and try and find strategies that work.

53
0:04:32,540 --> 0:04:38,300
 Because just saying no to saving the kids is not going to appeal to sort of the masses

54
0:04:38,340 --> 0:04:44,060
 And we need to make sure that we have technology that works, technical arguments for why the

55
0:04:44,060 --> 0:04:48,500
 proposals that they have don't work, but at the same time saying we're actually doing

56
0:04:48,500 --> 0:04:53,220
 this to help the kids as well.

57
0:04:53,220 --> 0:05:00,300
 So the next couple of minutes will be a quick outline of what Check Control is about and

58
0:05:00,300 --> 0:05:02,980
 also revealing the real name.

59
0:05:02,980 --> 0:05:05,940
 So Check Control is just what it's called on social media.

60
0:05:05,940 --> 0:05:10,020
 I think a phrase coined by Patrick Breyer of the German Pirate Party.

61
0:05:10,020 --> 0:05:16,780
 So the full name is Regulation Laying Down Rules to Prevent and Combat Sexual Abuse.

62
0:05:16,780 --> 0:05:25,060
 At IDRI, Isipol is a member of IDRI and I participated in IDRI's work on this file.

63
0:05:25,060 --> 0:05:33,100
 We call it the CSA Regulation or CSAR.

64
0:05:33,140 --> 0:05:36,820
 And that is, obviously we don't have time to go through everything and that would be

65
0:05:36,820 --> 0:05:44,580
 incredibly boring, so I'll just focus on the most interesting parts or the most troublesome

66
0:05:44,580 --> 0:05:48,900
 parts of the CSA Regulation.

67
0:05:48,900 --> 0:05:55,900
 And clearly the most, the absolutely most troublesome part is the Article 7-11 on Detection

68
0:05:55,900 --> 0:06:01,260
 Orders which is sort of basically why it's called Check Control.

69
0:06:01,260 --> 0:06:07,420
 These can apply to messenger services or interpersonal communication services, as they

70
0:06:07,420 --> 0:06:11,740
 are officially called, and hosting services.

71
0:06:11,740 --> 0:06:16,820
 And so messenger services are always private communication services, hosting services can

72
0:06:16,820 --> 0:06:21,500
 be public services like social media, but it can also be really private services like

73
0:06:21,500 --> 0:06:29,140
 cloud storage on Google Drive, Microsoft OneDrive and whatever it's called.

74
0:06:29,420 --> 0:06:34,900
 And if a service provider is issued a so-called Detection Order, that will be an obligation

75
0:06:34,900 --> 0:06:40,980
 to scan all users' content for three different things depending on which type of order is

76
0:06:40,980 --> 0:06:41,980
 given.

77
0:06:41,980 --> 0:06:49,500
 Known child sexual abuse material, which will be done using some hashing matching algorithm

78
0:06:49,500 --> 0:06:51,980
 or perceptual hashing typically.

79
0:06:51,980 --> 0:06:57,820
 But there are also possibilities for detection orders for unknown child sexual abuse material

80
0:06:57,820 --> 0:07:04,220
 and grooming, and the latter two will be done through some form of AI classifiers that

81
0:07:04,220 --> 0:07:10,180
 come with extremely high error rates.

82
0:07:10,180 --> 0:07:14,780
 These orders, the detection orders cannot be issued to all services, there's a requirement

83
0:07:14,780 --> 0:07:20,620
 of significant risk of child sexual abuse after the so-called mitigation measures that

84
0:07:20,620 --> 0:07:23,140
 I'll talk about in a minute.

85
0:07:23,140 --> 0:07:29,700
 But the way that significant risk is defined in the regulation itself will very likely

86
0:07:29,700 --> 0:07:36,300
 in practice mean that a lot of services will have a significant risk that they can be used

87
0:07:36,300 --> 0:07:42,940
 or abused for something bad involving children, especially as the very purpose of messenger

88
0:07:42,940 --> 0:07:48,500
 services is that people can communicate privately without the service provider monitoring what

89
0:07:48,500 --> 0:07:53,980
 people are talking about, and that means it will actually be very difficult for the

90
0:07:53,980 --> 0:07:59,860
 service provider to say that there is not a significant risk of the service being used

91
0:07:59,860 --> 0:08:03,260
 for child sexual abuse.

92
0:08:03,260 --> 0:08:07,940
 One interesting thing is to say, well, what about encrypted services, which are specifically

93
0:08:07,940 --> 0:08:13,700
 designed so that it's impossible to scan the content of people's communication?

94
0:08:14,220 --> 0:08:20,860
 Well, encryption is barely mentioned in the proposal itself, but it is presented as technologically

95
0:08:20,860 --> 0:08:25,500
 neutral, which means that it will simply be up to the service provider to figure out how

96
0:08:25,500 --> 0:08:32,380
 to do the impossible of scanning communications content that is end-to-end encrypted.

97
0:08:32,380 --> 0:08:42,100
 This is not explained particularly well in the proposal itself, but there was a Q&A published

98
0:08:42,100 --> 0:08:48,420
 by the Commission along with the proposal where it is spelled out that detection orders

99
0:08:48,420 --> 0:08:57,620
 will be technologically neutral, and it is an obligation of results, not of means.

100
0:08:57,620 --> 0:09:03,620
 So unlike sort of previous crypto wars where a specific circumvention of encryption was

101
0:09:03,620 --> 0:09:09,540
 proposed, this time it will be left to the service provider to backdoor their own services

102
0:09:09,540 --> 0:09:12,260
 essentially.

103
0:09:12,260 --> 0:09:16,580
 I'm skipping some of this for reasons of time because we want to get to the interesting

104
0:09:16,580 --> 0:09:23,060
 stuff what can be done about these bad things, and we will distribute the slides afterwards,

105
0:09:23,060 --> 0:09:24,900
 so don't worry about that.

106
0:09:24,900 --> 0:09:32,100
 I also want to talk a bit about risk assessment and risk mitigation, which is something that

107
0:09:32,100 --> 0:09:37,300
 is supposed to come before detection orders can be issued.

108
0:09:37,300 --> 0:09:44,220
 This is sort of a mixed bag of something that is, some is good, some is bad.

109
0:09:44,220 --> 0:09:52,180
 So taking the positive hat on is about making services platforms safe for children, which

110
0:09:52,180 --> 0:09:59,900
 is an admirable objective and a good idea, and if this is something that service providers

111
0:09:59,900 --> 0:10:06,660
 haven't done already, then having an obligation in EU law to do it is a good idea.

112
0:10:06,660 --> 0:10:12,460
 The problem with this is that it's done in a way that presents the internet as inherently

113
0:10:12,460 --> 0:10:17,940
 dangerous, and if something is inherently dangerous, we should keep children away from

114
0:10:17,940 --> 0:10:19,380
 it.

115
0:10:19,380 --> 0:10:24,820
 So just to walk you through what will happen, service providers will first be required to

116
0:10:24,820 --> 0:10:31,340
 assess the risk of their services being used for child sexual abuse, which is a difficult

117
0:10:32,180 --> 0:10:37,300
 exercise, and then they should take mitigation measures.

118
0:10:37,300 --> 0:10:41,780
 The problem with this is that it's rather vaguely defined, except the notion that the

119
0:10:41,780 --> 0:10:47,980
 internet is a dangerous place, and the only thing that is really concrete is that online

120
0:10:47,980 --> 0:10:54,380
 age verification is a good idea in the eyes of the Commission.

121
0:10:54,380 --> 0:11:01,260
 But some incentives are created to make services safer, which, so maybe having less sharing

122
0:11:01,260 --> 0:11:05,220
 of personal data by default, especially for accounts belonging to children, that would

123
0:11:05,220 --> 0:11:09,860
 be a great idea, but that is actually not in the Commission proposal, but in some of

124
0:11:09,860 --> 0:11:12,940
 the amendments that are coming out.

125
0:11:12,940 --> 0:11:22,100
 And then the heavily contested idea of age verification that is also coming up in national

126
0:11:22,100 --> 0:11:30,380
 politics in Denmark, Germany, the UK, with the online safety bill, and France, and maybe

127
0:11:30,380 --> 0:11:37,780
 potentially other countries, that is a not so good idea.

128
0:11:37,780 --> 0:11:40,900
 And by the way, there's also, I don't really have time to go into that, there's also an

129
0:11:40,900 --> 0:11:46,700
 age verification requirement for app stores, which will apply to all app stores, whether

130
0:11:46,700 --> 0:11:53,500
 they are large, like Apple's app store, or Google Play app store, or small, such as FDroid,

131
0:11:53,500 --> 0:11:59,220
 the alternative app store for Android devices.

132
0:11:59,220 --> 0:12:04,100
 And since these alternative app stores don't really know their users, they have no way

133
0:12:04,100 --> 0:12:07,260
 of enforcing age verification.

134
0:12:07,260 --> 0:12:11,940
 So the problem with online age verification, which I don't really have time to go into,

135
0:12:11,940 --> 0:12:16,540
 is that there's no good way to do it.

136
0:12:16,540 --> 0:12:25,020
 But it is being proposed, and typically it involves bad solutions, such as uploading

137
0:12:25,020 --> 0:12:32,820
 copies of ID documents, which sort of invites identity theft, doing some form of biometric

138
0:12:32,820 --> 0:12:40,060
 facial analysis, a company called Yoti is heavily invested in doing that, which is at

139
0:12:40,060 --> 0:12:46,780
 best an estimation of your age, not a verification of your age, checks your payment cards, and

140
0:12:46,780 --> 0:12:51,620
 then maybe 10, 15 years from now some really fancy solutions where you can prove your age

141
0:12:51,860 --> 0:12:58,380
 anonymously without revealing anything, but this is nowhere ready for deployment.

142
0:12:58,380 --> 0:13:03,620
 Irrespective of what comes out of this, there's a risk of digital exclusion.

143
0:13:03,620 --> 0:13:07,460
 Users that will not be able to comply with these age verification requirements, whether

144
0:13:07,460 --> 0:13:12,700
 they are children or adults, will be locked out of online services that they want to use,

145
0:13:12,700 --> 0:13:19,140
 potentially even including Digipedia.

146
0:13:19,140 --> 0:13:25,180
 We thought we'd give you a quick outline of how legislation in the European Union works.

147
0:13:25,180 --> 0:13:33,780
 If you study political science or law, you will spend years of classes and 10 or 20 ECTS

148
0:13:33,780 --> 0:13:38,820
 points on this, so we're going to do it very briefly in basically one slide.

149
0:13:38,820 --> 0:13:44,260
 The Commission comes with a proposal, then both the Member States and the Council and

150
0:13:44,260 --> 0:13:48,100
 us in the European Parliament, we look at it together.

151
0:13:48,100 --> 0:13:53,580
 The Commission came with a proposal last year in May and already at last year's Born Hack

152
0:13:53,580 --> 0:13:57,940
 there was a group of us that got together and said, this looks really bad, let's try

153
0:13:57,940 --> 0:13:59,580
 and do something about it.

154
0:13:59,580 --> 0:14:04,660
 That's why we have, together with Jorn, who talked yesterday, we have set up a website

155
0:14:04,660 --> 0:14:10,780
 called chatcontrol.dk and .eu and we also have a mailing list, so please support us

156
0:14:10,780 --> 0:14:18,020
 on working on that so that we can influence both Member States and the Parliamentarians.

157
0:14:18,020 --> 0:14:23,340
 We are in the Parliament in the process of making our amendments to the Commission proposal

158
0:14:23,340 --> 0:14:29,140
 and we'll probably have a vote here in the fall on this and the Member States are also

159
0:14:29,140 --> 0:14:37,740
 making their amendments and trying to have a joint position and that's where the advocating

160
0:14:37,740 --> 0:14:45,980
 for National Parliaments come in and then after both the Member States and the Parliament

161
0:14:46,020 --> 0:14:51,100
 have come with their positions, then you go into what's called a trilogue negotiations

162
0:14:51,100 --> 0:14:58,580
 where we sit down together with the Commission, the Presidency of the European Council, so

163
0:14:58,580 --> 0:15:00,780
 that's going to be Belgium.

164
0:15:00,780 --> 0:15:05,420
 That was Belgium this spring and then now it's Spain and then you'll have Hungary and

165
0:15:05,420 --> 0:15:10,660
 Poland in the next coming six-month periods and that's when they sit in a closed room

166
0:15:10,660 --> 0:15:15,700
 and try to hash things out and figure out how the legislation will look like.

167
0:15:16,620 --> 0:15:22,460
 As Jesper has written down, it's typically somewhere between the Parliament and the Council position.

168
0:15:24,100 --> 0:15:27,980
 Jesper is going to talk us through the way it looks in the Council between all of the

169
0:15:27,980 --> 0:15:28,980
 Member States.

170
0:15:28,980 --> 0:15:32,180
 I'll do the bad guys and Karen will do the good guys.

171
0:15:32,180 --> 0:15:36,460
 That's how we divided the EU legislature on this file.

172
0:15:36,460 --> 0:15:42,660
 No, just kidding, actually a couple of months ago I was more optimistic about Council than

173
0:15:42,660 --> 0:15:48,300
 Parliament, but there was a bombshell event in April that changed a lot in Parliament

174
0:15:48,300 --> 0:15:51,740
 but not in Council.

175
0:15:51,740 --> 0:16:01,260
 So a working group, the Law Enforcement Working Party, and don't be confused by the word,

176
0:16:01,260 --> 0:16:04,660
 it has nothing to do with parties, it's sort of a working group that is always called

177
0:16:04,660 --> 0:16:05,660
 working parties.

178
0:16:05,660 --> 0:16:15,700
 The Law Enforcement Working Party has worked on this proposal since June last year and

179
0:16:15,700 --> 0:16:23,060
 they are, the plan they have announced is that on 28 September they will adopt their

180
0:16:23,060 --> 0:16:24,060
 position.

181
0:16:24,060 --> 0:16:32,540
 This has to be done at a meeting where the Ministers are present, the Interior and Justice

182
0:16:32,540 --> 0:16:39,260
 Ministers, so they have a Justice and Home Affairs Council which will be held on the

183
0:16:39,260 --> 0:16:47,940
 28th September where their plan is to adopt the Council position which becomes the negotiating

184
0:16:47,940 --> 0:16:53,900
 position for the EU Member States when they get together with the European Parliament

185
0:16:53,900 --> 0:16:57,740
 in the trilogues that Karen talked about.

186
0:16:57,740 --> 0:17:05,700
 And looking at the most recent Council text, it has a lot of amendments, but it has very

187
0:17:05,700 --> 0:17:08,600
 few amendments to detection orders.

188
0:17:08,600 --> 0:17:14,540
 They still apply general and indiscriminately to all users irrespective of whether they

189
0:17:14,540 --> 0:17:20,080
 are suspected of actual involvement in child sexual abuse or not.

190
0:17:20,080 --> 0:17:29,620
 So the only thing that Council saw a need to exclude was real-time voice calls.

191
0:17:29,620 --> 0:17:36,360
 Everything else including encrypted services are still included in this group of the proposal.

192
0:17:36,360 --> 0:17:44,360
 There was a couple of weeks ago under the former Swedish Presidency a proposal to exclude

193
0:17:44,360 --> 0:17:50,340
 or not necessarily exclude encrypted services from the proposal, but to protect them from

194
0:17:50,340 --> 0:17:56,760
 detection orders and at least make sure that encryption is not undermined, and this was

195
0:17:56,760 --> 0:18:01,920
 framed in a way that clients are scanning that I'm sure Jan talked about yesterday,

196
0:18:01,920 --> 0:18:07,780
 planting the spyware on your mobile phone, would be prohibited.

197
0:18:07,780 --> 0:18:13,000
 But there was no support for this proposal even though Germany and a couple of other

198
0:18:13,000 --> 0:18:18,560
 member states have advocated quite heavily for it, so that was dropped again.

199
0:18:18,560 --> 0:18:22,760
 There are also fairly modest changes on risk assessment and risk mitigation, meaning age

200
0:18:22,760 --> 0:18:29,400
 verification is still de facto mandatory in certain cases, and then Council made a lot

201
0:18:29,400 --> 0:18:37,480
 of changes to some of the more boring aspects of the CSA regulation that I'll not talk about.

202
0:18:37,480 --> 0:18:45,440
 One interesting thing is that they decided to completely ignore a legal opinion that

203
0:18:45,440 --> 0:18:50,840
 the member states themselves requested from the Council Legal Service, the specialist

204
0:18:50,840 --> 0:18:59,480
 lawyers employed to advise member states on whether a proposal complies with EU primary

205
0:18:59,480 --> 0:19:03,580
 law including the Charter of Fundamental Rights.

206
0:19:03,580 --> 0:19:09,040
 They don't usually ask for such an opinion because there's a presumption that new laws

207
0:19:09,040 --> 0:19:14,900
 are consistent with the constitution of the EU, but in this case they did, and they got

208
0:19:14,900 --> 0:19:20,660
 a very unusual opinion back at the end of April which clearly said, no, this is not

209
0:19:20,660 --> 0:19:26,220
 consistent, this does not comply with the Charter of Fundamental Rights.

210
0:19:26,220 --> 0:19:31,940
 And I must say at ITERI we have done our own Fundamental Rights Assessment and that was

211
0:19:32,300 --> 0:19:37,020
 actually quite lacking, or at least not quite lacking, that was sort of less complete than

212
0:19:37,020 --> 0:19:41,780
 the one that the Council Legal Service did and was supposed to be the radicals on fundamental

213
0:19:41,780 --> 0:19:45,380
 rights, but in this case we were not.

214
0:19:45,380 --> 0:19:50,660
 So just to sort of quickly summarize, they used the data retention case law, we did that

215
0:19:50,660 --> 0:19:56,420
 as well and concluded that general indiscriminate detection orders are not limited to what is

216
0:19:56,420 --> 0:20:01,620
 strictly necessary, which is sort of the yardstick for doing something in compliance with fundamental

217
0:20:01,620 --> 0:20:04,980
 rights at the EU level.

218
0:20:04,980 --> 0:20:10,260
 They also, and this is where it gets really unusual, said that the proposal is likely

219
0:20:10,260 --> 0:20:18,340
 to violate the essence of the right to privacy, and that is the generalized scanning of content

220
0:20:18,340 --> 0:20:28,300
 of communications for everybody, goes back to the first SRAMS judgment in 2015 where

221
0:20:28,460 --> 0:20:35,980
 they were referring to the National Security Agency, the Court of Justice, and basically

222
0:20:35,980 --> 0:20:41,140
 they pointed out that this would create a state of sort of a permanent surveillance

223
0:20:41,140 --> 0:20:47,260
 society where all interpersonal communications are monitored.

224
0:20:47,260 --> 0:20:52,300
 And the lawyers, the clever lawyers, pointed out that detection orders must be targeted

225
0:20:52,300 --> 0:20:57,460
 against individuals where there's actual suspicion of involvement in child sexual abuse, not

226
0:20:57,620 --> 0:21:00,180
 the general population.

227
0:21:00,180 --> 0:21:05,900
 Very interesting, they also pointed out that doing this on end-to-end encrypted services

228
0:21:05,900 --> 0:21:10,420
 where they use the word backdoor, although it doesn't really have a legal meaning, creates

229
0:21:10,420 --> 0:21:18,780
 an additional interference with a right to data security, which is very interesting.

230
0:21:18,780 --> 0:21:23,540
 To make a long story short, the Commission response to all this was that, well, the content

231
0:21:23,620 --> 0:21:28,940
 is the crime, so we need to monitor everybody's communication, which is a completely untested

232
0:21:28,940 --> 0:21:33,180
 legal argument, so it will be interesting if this ever gets to the Court of Justice,

233
0:21:33,180 --> 0:21:38,660
 what the justice will say to the Commission.

234
0:21:38,660 --> 0:21:48,500
 But member states are seemingly set to ignore this altogether, so what can possibly stop

235
0:21:49,460 --> 0:21:56,780
 a disaster with the CSA regulation in Council?

236
0:21:56,780 --> 0:22:02,980
 A better negotiating position for member states that does not include mass surveillance?

237
0:22:02,980 --> 0:22:12,500
 Well, if there is a blocking minority for member states representing at least 35% of

238
0:22:12,500 --> 0:22:16,420
 the EU population, it's not enough to have a majority in Council that needs to be a super

239
0:22:16,500 --> 0:22:24,460
 majority of 65% of the EU population, but if there's a blocking minority, they can,

240
0:22:24,460 --> 0:22:32,140
 in principle, stop the position in its current form going forward.

241
0:22:32,140 --> 0:22:37,100
 And we know there's substantial disagreement in the law enforcement working party about

242
0:22:37,100 --> 0:22:40,780
 detection orders in the child sexual abuse regulation.

243
0:22:40,780 --> 0:22:45,500
 That is why they requested the legal opinion from the Council legal service.

244
0:22:45,580 --> 0:22:51,100
 It is also why they sort of escalated their disagreement to the ambassador level in the

245
0:22:51,100 --> 0:22:59,860
 Corp here, I'm not pronouncing this correctly, in the end of May, but the ambassadors basically

246
0:22:59,860 --> 0:23:08,380
 reaffirmed we want to monitor everybody, potentially even including all your communication.

247
0:23:08,380 --> 0:23:14,140
 Some of the good guys, the good member states include Germany, Austria, the Netherlands,

248
0:23:14,140 --> 0:23:19,580
 possibly Poland and Estonia, who have all in various forms voiced criticism of this,

249
0:23:19,580 --> 0:23:27,460
 and then there's a like-minded group of other countries, Belgium, Bulgaria, and so forth,

250
0:23:27,460 --> 0:23:30,900
 that want the proposal in its current form.

251
0:23:30,900 --> 0:23:36,180
 If you're wondering what does Denmark think about all this, well, Denmark has the most

252
0:23:36,180 --> 0:23:37,980
 absurd position.

253
0:23:37,980 --> 0:23:42,660
 Denmark wants end-to-end encrypted services to be included in the proposal, but they do

254
0:23:42,660 --> 0:23:47,100
 not want, Denmark does not want to weaken encryption in any way, and it's a bit hard

255
0:23:47,100 --> 0:23:55,420
 to sort of monitor and get access to encrypted communication without weakening encryption

256
0:23:55,420 --> 0:24:01,140
 in some way, because the whole point of encryption is to prevent this monitoring.

257
0:24:01,140 --> 0:24:08,660
 So the big question now going forward is will the majority among the member states be able

258
0:24:08,660 --> 0:24:15,780
 to pressure the minority to give up its opposition so that council reads it as a compromise?

259
0:24:15,780 --> 0:24:24,340
 That is often how it's played out, because sort of the way council works is seeking compromises.

260
0:24:24,340 --> 0:24:30,420
 But this really needs to be seen, and now I'll hand over to Karen, because we also have

261
0:24:30,420 --> 0:24:32,060
 the good guys in the European Parliament.

262
0:24:32,060 --> 0:24:33,940
 Well, sort of the good guys.

263
0:24:33,940 --> 0:24:37,160
 We have, in the European Parliament, we have a lot of different committees.

264
0:24:37,160 --> 0:24:41,200
 The main committee is our Fundamental Rights Committee, which is called LIBE.

265
0:24:41,200 --> 0:24:49,960
 They are the ones sort of writing the main opinion from the Parliament regarding the proposal.

266
0:24:49,960 --> 0:24:56,400
 And we have a bunch of other committees, including the FAM Committee, which is for equality,

267
0:24:56,400 --> 0:24:58,880
 and also the Cult and the Budget opinion.

268
0:24:58,880 --> 0:25:05,280
 We write opinions and let the LIBE committee know what we think that they should include.

269
0:25:05,360 --> 0:25:13,000
 They can usually just disregard what we're saying and just continue on with what they're thinking.

270
0:25:13,000 --> 0:25:19,200
 We've had a draft report from the LIBE rapporteur in April, where they do have protection of

271
0:25:19,200 --> 0:25:23,560
 message content for end-to-end encryption, but this is a little bit like, I don't know

272
0:25:23,560 --> 0:25:29,200
 if you recall, the copyright directive debate, where they said, well, we're not going to

273
0:25:29,200 --> 0:25:34,080
 have upload filters, but we're going to ask you as a service provider, as a platform,

274
0:25:34,080 --> 0:25:38,520
 to do something that you can only do with an upload filter.

275
0:25:38,520 --> 0:25:43,800
 We're not asking you to have it, but we're asking you to provide us with an end result

276
0:25:43,800 --> 0:25:46,800
 that requires you to have an upload filter.

277
0:25:46,800 --> 0:25:53,280
 And this is a little bit what the problem is with the debate in this Crypto War 4.0,

278
0:25:53,280 --> 0:25:58,440
 which is going to be the next debate as well here in the Bonhac, is that they're not actually

279
0:25:58,440 --> 0:26:03,560
 saying precisely what it is that they want the service providers to be doing, because

280
0:26:03,560 --> 0:26:05,480
 they know that's going to be unpopular.

281
0:26:05,480 --> 0:26:10,760
 So they just sort of fudge it and say, we're going to ask you to provide us with this end

282
0:26:10,760 --> 0:26:14,720
 result, but we're not going to tell you where it is.

283
0:26:14,720 --> 0:26:22,160
 And we've had also in the draft report from the LIBE committee voluntary detection orders,

284
0:26:22,160 --> 0:26:29,760
 which is sort of trying to improve the position, the proposal that we came from the commission,

285
0:26:29,760 --> 0:26:33,200
 so instead of having it mandatory, then it's just voluntary.

286
0:26:33,200 --> 0:26:37,480
 And also we're going to have supporting age verification.

287
0:26:37,480 --> 0:26:42,760
 We're hoping that it's going to have not last 10, 15 years before we have something, but

288
0:26:42,760 --> 0:26:50,360
 we have sort of EU wallet where you'll be able to have age verification without sharing

289
0:26:50,360 --> 0:26:52,000
 your identity.

290
0:26:52,000 --> 0:26:57,800
 And also different mitigation measures so that you don't need to have detection orders.

291
0:26:57,800 --> 0:27:03,080
 In the FAM committee where I was what's called the shadow rapporteur from the renew group,

292
0:27:03,120 --> 0:27:09,240
 we actually had a lot of proposals on how to improve the proposal, so amendments, but

293
0:27:09,240 --> 0:27:14,480
 unfortunately they said that this was too technical for the Equality Committee to look

294
0:27:14,480 --> 0:27:19,200
 at, so they said that they were out of scope for the FAM opinion.

295
0:27:19,200 --> 0:27:26,520
 So this was also we have a Swedish rapporteur in the committee, and the commissioner that

296
0:27:26,520 --> 0:27:30,640
 has put the proposal forward is also a Swedish commissioner.

297
0:27:30,680 --> 0:27:36,440
 And this is like her pièce de résistance, her big thing that she wants to achieve in

298
0:27:36,440 --> 0:27:39,880
 this mandate of her being a commissioner.

299
0:27:39,880 --> 0:27:42,800
 So there's a lot of personal pride and ego in this.

300
0:27:42,800 --> 0:27:49,120
 So the Swedish rapporteur in the FAM committee managed to get all of our amendments taken

301
0:27:49,120 --> 0:27:55,520
 off the table, so they weren't included in the FAM opinion.

302
0:27:55,560 --> 0:28:01,440
 Nespar already mentioned the Council's legal service, and that they had an opinion saying

303
0:28:01,440 --> 0:28:06,400
 basically you can't do this and respect our Charter of Fundamental Rights, so basically

304
0:28:06,400 --> 0:28:10,680
 you're going against the sort of fundamental laws of the European Union if you want to

305
0:28:10,680 --> 0:28:12,920
 go forward with this proposal.

306
0:28:12,920 --> 0:28:18,480
 We in the Parliament asked our lawyers, the Parliament legal service, to look at the proposal,

307
0:28:18,480 --> 0:28:25,000
 and they came up with the same conclusion that this proposal is garbage and you shouldn't

308
0:28:25,040 --> 0:28:31,080
 adopt it, and it should actually be taken away and looked at again.

309
0:28:31,080 --> 0:28:39,320
 Unfortunately, the people who've invested a lot of private pride and commitment to this

310
0:28:39,320 --> 0:28:42,360
 are actually still going ahead.

311
0:28:42,360 --> 0:28:48,920
 So we have, this is the complementary impact assessments, where we're also saying, okay,

312
0:28:48,920 --> 0:28:51,800
 you cannot technically have detection orders.

313
0:28:51,800 --> 0:28:58,160
 You can only do that for known CSAM, and even then, if you're doing it with hash, you can

314
0:28:58,160 --> 0:29:01,240
 circumvent that, and it doesn't actually work.

315
0:29:01,240 --> 0:29:07,600
 The problem with the proposal from the Commission, in my opinion, is that you're trying to use

316
0:29:07,600 --> 0:29:13,320
 technology to protect kids that is not actually going to protect them because it can be circumvented,

317
0:29:13,320 --> 0:29:21,240
 whether we're talking about hashing for known CSAM, but also if you're looking at age verification,

318
0:29:21,280 --> 0:29:26,400
 for example, in app stores, then you can find the apps somewhere outside of the app store

319
0:29:26,400 --> 0:29:28,760
 and then upload it to your device.

320
0:29:28,760 --> 0:29:35,280
 We have amendments from LIBE that are actually looking quite good with protection of end-to-end

321
0:29:35,280 --> 0:29:41,000
 encryption and also that the targeted detection orders are actually going against persons

322
0:29:41,000 --> 0:29:47,160
 that are suspected in involvement of child sexual abuse, where we're saying, okay, you

323
0:29:47,240 --> 0:29:52,640
 can actually detect stuff, but only if you have a court order looking at it, if there

324
0:29:52,640 --> 0:30:00,000
 is an investigation going on, because I usually say if you want to find a needle in a haystack,

325
0:30:00,000 --> 0:30:04,400
 you shouldn't make the haystack bigger, but you should actually find a magnet.

326
0:30:04,400 --> 0:30:11,800
 If you are going to have detection orders and scanning of all of our communication,

327
0:30:11,800 --> 0:30:14,400
 you're going to be making the haystack bigger.

328
0:30:14,400 --> 0:30:18,880
 And then also, I mean, we are a parliament that's quite split on this, so there are also

329
0:30:18,880 --> 0:30:23,480
 some bad amendments on, for example, voluntary detection.

330
0:30:23,480 --> 0:30:31,440
 And the situation in the parliament is that we are very split on this proposal, and unfortunately

331
0:30:31,440 --> 0:30:36,760
 a lot of the people that are the rapporteurs or the shadows are quite pro the proposal,

332
0:30:36,760 --> 0:30:42,480
 a little bit the situation that I was talking about in the FAM Committee, where the person

333
0:30:42,560 --> 0:30:47,760
 is writing the opinion like the proposal and is not listening to the criticism.

334
0:30:47,760 --> 0:30:53,880
 So therefore, it's really important not only to influence the people that are the rapporteurs

335
0:30:53,880 --> 0:30:59,820
 or the shadows of the proposal, but also their wider group.

336
0:30:59,820 --> 0:31:08,160
 So the next steps are going to be that there are compromise amendments for the Liebe report,

337
0:31:08,840 --> 0:31:16,680
 and that works as the rapporteur that we looked at here from Spain and the conservative groups,

338
0:31:16,680 --> 0:31:19,480
 my Spanish is not good enough to pronounce that.

339
0:31:19,480 --> 0:31:26,480
 He will negotiate with the other political groups about what the amendments that have

340
0:31:26,480 --> 0:31:32,640
 been put forward and trying to find out if they can reach a compromise on the amendments.

341
0:31:33,120 --> 0:31:38,840
 Once they have those, they are going to be put to a vote in the Liebe Committee, and

342
0:31:38,840 --> 0:31:44,920
 then after that they are going to be put to a vote in the plenary where all 705 of us,

343
0:31:44,920 --> 0:31:50,160
 or a little bit less, and people don't care to show up, are going to vote on the proposal

344
0:31:50,160 --> 0:31:51,160
 as a whole.

345
0:31:51,160 --> 0:31:59,120
 Liebe has been postponed until October, and then after that there will be a vote in the

346
0:31:59,120 --> 0:32:02,120
 plenary.

347
0:32:02,600 --> 0:32:06,840
 So if that's going to be in October, possibly later, it will probably be in the spring just

348
0:32:06,840 --> 0:32:10,920
 before the election of the next European Parliament.

349
0:32:10,920 --> 0:32:17,800
 That means that there is a lot of personal interest of the different parliamentarians

350
0:32:17,800 --> 0:32:24,400
 in the result coming out, because they need to go out and defend it only weeks later in

351
0:32:24,400 --> 0:32:30,680
 the general public, and I think that is a real big opportunity for us to actually influence

352
0:32:30,760 --> 0:32:36,120
 the people in parliament, to say, are you able to go out and meet your voters and defend

353
0:32:36,120 --> 0:32:38,280
 what you voted through?

354
0:32:38,280 --> 0:32:46,280
 But that means that we need to actually try and influence not just the people in the specific

355
0:32:46,280 --> 0:32:49,280
 committee, but more widely.

356
0:32:49,280 --> 0:32:58,400
 In Renew, for example, we have a couple of members from the FDP in Germany, and Germany,

357
0:32:58,520 --> 0:33:02,720
 even the government, is against the proposal, as Jesper was saying, and the debates that

358
0:33:02,720 --> 0:33:08,400
 we've been having internally in the group has actually shifted the position of our rapporteur,

359
0:33:08,400 --> 0:33:15,080
 who is a Belgian lady, who is very pro the proposal.

360
0:33:15,080 --> 0:33:22,720
 And there has been a lot of lobbying from companies that can provide technology for

361
0:33:22,800 --> 0:33:29,600
 doing client-signed scanning and scanning in general, and also Ashton Krishna, who will

362
0:33:29,600 --> 0:33:33,440
 be appearing later in the slides as well.

363
0:33:33,440 --> 0:33:35,560
 And the S&D is also split.

364
0:33:35,560 --> 0:33:44,080
 They have a large German fraction in the political group, and they are trying to then also shift

365
0:33:44,080 --> 0:33:51,520
 the position of the S&D there, but there they have the problem that the commissioner, she

366
0:33:51,520 --> 0:33:57,160
 is a Swedish social democrat, and then how do you then balance supporting your political

367
0:33:57,160 --> 0:34:02,920
 family commissioner and having good legislation?

368
0:34:02,920 --> 0:34:09,760
 So I will very quickly sort of outline who is trying to influence the legislative process,

369
0:34:09,760 --> 0:34:13,320
 what is sometimes called lobbying.

370
0:34:13,360 --> 0:34:21,080
 We have sort of free groups of, free advocacy groups, civil society organizations like IDRI,

371
0:34:21,080 --> 0:34:30,400
 Electronic Frontier Foundation, CGT, and others, it's not an exhaustive list.

372
0:34:30,400 --> 0:34:36,560
 They tend to sort of oppose the main elements of the child sexual abuse regulation, decision

373
0:34:36,560 --> 0:34:41,400
 orders, and age identification that I've talked about, and at IDRI we have also called for

374
0:34:41,480 --> 0:34:45,400
 the entire proposal to be rejected because children deserve better.

375
0:34:45,400 --> 0:34:49,640
 So if you take detection orders out of her proposal that is basically all about mass

376
0:34:49,640 --> 0:34:55,600
 surveillance, there's very little depth doing something for children online.

377
0:34:55,600 --> 0:35:01,040
 So opposing us are mainly the child protection organization, or I should say some of them

378
0:35:01,040 --> 0:35:07,800
 are, some of them are critical, but a majority of child protection organizations throughout

379
0:35:07,800 --> 0:35:17,720
 Europe support the proposal, and they are sort of the most, STON supporters are really

380
0:35:17,720 --> 0:35:24,800
 an organization called STON led by Aston Kuster who pushes for technological solutions and

381
0:35:24,800 --> 0:35:29,720
 goes to the European Parliament and gives movie star explanations of how technology

382
0:35:29,720 --> 0:35:30,720
 works.

383
0:35:30,720 --> 0:35:36,440
 For instance, that Aston Kuster's scanning technology cannot be used for anything else

384
0:35:36,440 --> 0:35:41,880
 than child sexual abuse material, so there's absolutely no risk of mission creep.

385
0:35:41,880 --> 0:35:47,440
 We're talking about matching hash values, and these hash values can be anything, but

386
0:35:47,440 --> 0:35:50,560
 that is sort of forgotten.

387
0:35:50,560 --> 0:35:57,440
 Big tech service providers, Google, Meta, Apple, and so forth are also involved.

388
0:35:57,440 --> 0:36:04,000
 They tend to oppose mandatory measures, but they like voluntary measures when nobody tells

389
0:36:04,000 --> 0:36:06,040
 them what to do.

390
0:36:06,040 --> 0:36:13,840
 And sort of the most principled opposition that has come out is an opposition to weakening

391
0:36:13,840 --> 0:36:18,480
 encryption just because a law in the United Kingdom or in the European Union requires

392
0:36:18,480 --> 0:36:20,560
 it because this will have global effects.

393
0:36:20,560 --> 0:36:27,960
 So WhatsApp, Signal, and Apple, I forgot to add that to the slide, have said that in principle

394
0:36:27,960 --> 0:36:31,640
 they will refuse to comply with such demands.

395
0:36:31,640 --> 0:36:35,680
 Then maybe one day Signal will be blocked in the European Union, just like Signal is

396
0:36:36,320 --> 0:36:39,440
 blocked in Iran, and we'll have to use it for proxies, who knows.

397
0:36:39,440 --> 0:36:41,480
 That could be what we're looking at.

398
0:36:41,480 --> 0:36:45,760
 This is just a dude, where is my car?

399
0:36:45,760 --> 0:36:51,640
 This is Maurice Koerner, one of Karen's colleagues in the Renew Group, making jokes about who

400
0:36:51,640 --> 0:36:53,000
 is advising the Commission.

401
0:36:53,000 --> 0:36:57,080
 It's actually more tragic than it's funny.

402
0:36:57,080 --> 0:37:02,080
 And this is just to give you an example of sort of how this lobbying activity works,

403
0:37:02,120 --> 0:37:07,080
 sending briefing papers to parliamentarians or rather to their assistants because they

404
0:37:07,080 --> 0:37:10,040
 don't have time to read them themselves.

405
0:37:10,040 --> 0:37:15,280
 So there was a fact check of top nine claims, blah, blah, blah, from one of the child protection

406
0:37:15,280 --> 0:37:22,200
 organizations, which was immediately followed by a fact checking of the fact checking done

407
0:37:22,200 --> 0:37:30,000
 by Idrie actually by myself because Ella Jakubowsky was on holiday.

408
0:37:30,000 --> 0:37:37,600
 I will, in view of time, I'll skip our Idrie advocacy efforts and just mention that the

409
0:37:37,600 --> 0:37:43,520
 Commission really has no response to the criticism from the tech community and criticism of

410
0:37:43,520 --> 0:37:47,680
 technical issues in general.

411
0:37:47,680 --> 0:37:53,080
 There was an open letter from the scientific community signed by more than 500 at present

412
0:37:53,080 --> 0:37:59,680
 scientists saying that detection technologies are deeply flawed and vulnerable to attack

413
0:37:59,680 --> 0:38:05,400
 and that undermining end-to-end encrypted services, backdooring them, will have severe

414
0:38:05,400 --> 0:38:06,920
 technical implications.

415
0:38:06,920 --> 0:38:09,600
 Well, what was the response from the Commission?

416
0:38:09,600 --> 0:38:15,520
 Well, she gave a speech a week later where she sort of compared that letter to a report

417
0:38:15,520 --> 0:38:20,600
 from a victim of child sexual abuse to be not happening online, but that's beside the

418
0:38:20,600 --> 0:38:21,600
 point.

419
0:38:22,360 --> 0:38:27,720
 So on one hand we have this report from that individual, on the other we have an article

420
0:38:27,720 --> 0:38:35,240
 that is the scientific letter that only speaks about technical hypotheticals and she's sort

421
0:38:35,240 --> 0:38:40,840
 of basically calling it conspiracy theories, which is totally insane because it's a well-founded

422
0:38:40,840 --> 0:38:44,920
 critique of the proposal.

423
0:38:44,920 --> 0:38:51,040
 And now Karen will tell you how you can stop this or help stopping this.

424
0:38:51,080 --> 0:38:56,080
 I think the last slide is quite evocative of the problem.

425
0:38:56,080 --> 0:39:02,120
 They're proposing a technical solution where the technology that they're pointing to simply

426
0:39:02,120 --> 0:39:03,960
 isn't going to be working.

427
0:39:03,960 --> 0:39:11,080
 At the same time you have sort of front organisations saying that they're working to protect children

428
0:39:11,080 --> 0:39:17,000
 against sexual abuse online, saying that this is going to help and it's the only way of

429
0:39:17,000 --> 0:39:20,040
 helping children against sexual abuse.

430
0:39:20,040 --> 0:39:24,640
 And therefore we need to say, OK, we want to actually help children against sexual abuse.

431
0:39:24,640 --> 0:39:31,240
 We need to get some of the other children's rights organisations on board on this and

432
0:39:31,240 --> 0:39:36,920
 get them to speak out about the problems that this proposal is going to have on protecting

433
0:39:36,920 --> 0:39:38,320
 kids.

434
0:39:38,320 --> 0:39:45,680
 Because I think if we just look at the very real technical criticism, then we're going

435
0:39:45,720 --> 0:39:51,160
 to lose a lot of people because they don't actually care about the technology and the

436
0:39:51,160 --> 0:39:53,840
 solutions that could be proposed or not.

437
0:39:53,840 --> 0:39:58,200
 Because it's like, oh, there is a debate on one side and the other side and then we'll

438
0:39:58,200 --> 0:40:01,240
 just ignore both of them.

439
0:40:01,240 --> 0:40:07,240
 There is the EDRIK campaign teams, both at a European level.

440
0:40:07,240 --> 0:40:12,680
 There is a working group, a campaign industry working group on a slide that just was skipped.

441
0:40:12,680 --> 0:40:14,520
 And then there are the national organisations.

442
0:40:14,520 --> 0:40:18,280
 I think one of the most important things you should do if you're a Dane sitting here

443
0:40:18,280 --> 0:40:22,760
 is go and actually become a member of IT politics funding.

444
0:40:22,760 --> 0:40:29,600
 It costs you 100 hacks a year and will help to sort of reinforce the organisation and

445
0:40:29,600 --> 0:40:35,200
 then also join the communication campaign on this.

446
0:40:35,200 --> 0:40:40,880
 At the national level, the current government and members of the parliament in a lot of

447
0:40:41,360 --> 0:40:47,200
 countries need to approve the decision made by the government when they go into the council.

448
0:40:47,200 --> 0:40:56,200
 So in Denmark, EU will have to approve the mandate for negotiations and the final vote

449
0:40:56,200 --> 0:41:03,000
 of the Danish government when they go to the vote on the 28th of September.

450
0:41:03,000 --> 0:41:09,200
 So therefore it's really important that you target, for example, the EU spokesperson in

451
0:41:09,240 --> 0:41:13,480
 the different political parties and also the person dealing with technology.

452
0:41:13,480 --> 0:41:16,960
 And in Denmark we even have a digitalisation minister now.

453
0:41:16,960 --> 0:41:26,760
 And try and explain to them in easy, tangible ways of how this proposal is not going to

454
0:41:26,760 --> 0:41:28,240
 be working.

455
0:41:28,240 --> 0:41:34,680
 In the European parliament, you could go in and then contact the individual MEPs, both

456
0:41:34,680 --> 0:41:37,560
 in Liebe, ahead of the vote in Liebe.

457
0:41:37,560 --> 0:41:43,640
 You can go in on the committee website and see who the members are.

458
0:41:43,640 --> 0:41:46,520
 Emails are already available.

459
0:41:46,520 --> 0:41:53,120
 You can contact us and try and provide us with arguments that we can then also bring

460
0:41:53,120 --> 0:41:56,640
 forward in our groups.

461
0:41:56,640 --> 0:42:06,240
 Individual emails are, of course, better than the sort of mass emailing systems where we

462
0:42:06,240 --> 0:42:09,760
 just put up a filter and put that in a folder or delete it.

463
0:42:09,760 --> 0:42:15,280
 Because we're not that technically smart, but we can work around Outlook, which is what

464
0:42:15,280 --> 0:42:17,560
 we're using.

465
0:42:17,560 --> 0:42:19,880
 Technical input is, of course, very important.

466
0:42:19,880 --> 0:42:23,160
 Let us know what doesn't work.

467
0:42:23,160 --> 0:42:30,120
 And then also I think it's really important to try and use personal, tangible and national

468
0:42:30,120 --> 0:42:31,440
 arguments.

469
0:42:31,520 --> 0:42:36,320
 We were discussing a little bit about whether we should actually use, mention the Rasmus

470
0:42:36,320 --> 0:42:42,280
 Pelluden and the sort of debate that we have in Denmark and Sweden about the Quran burnings.

471
0:42:42,280 --> 0:42:48,040
 But I mean, try and use the political debates that are in each member state and try and

472
0:42:48,040 --> 0:42:52,600
 get the journalists and the politicians involved.

473
0:42:52,600 --> 0:42:53,880
 Communicate about this.

474
0:42:53,880 --> 0:42:57,760
 There's a lot of people communicating about why they think it's a good proposal.

475
0:42:57,800 --> 0:43:03,760
 You need to help communicate the problems that are there with the proposal.

476
0:43:03,760 --> 0:43:09,160
 I think it's taking what Jesper is writing, for example, in IT police screening, and repeating

477
0:43:09,160 --> 0:43:14,360
 it on your own websites or wherever is going to be useful.

478
0:43:14,360 --> 0:43:20,280
 And use the platforms that are there that you know how to use to get people's attention.

479
0:43:20,280 --> 0:43:24,040
 We know that this is not working.

480
0:43:24,280 --> 0:43:29,880
 Google has had an automated tool to detect abusive images of children, but the system

481
0:43:29,880 --> 0:43:32,240
 is getting it wrong.

482
0:43:32,240 --> 0:43:38,920
 We need to make sure that people actually understand how amazing the internet is, what

483
0:43:38,920 --> 0:43:41,440
 it can be used for good things.

484
0:43:41,440 --> 0:43:47,280
 And also that it allows young persons and children to actually engage with communities

485
0:43:47,280 --> 0:43:52,240
 that they are a part of but might not be present where they're living geographically.

486
0:43:52,240 --> 0:43:58,480
 And also that we need to make sure that the technology that we use encourage children

487
0:43:58,480 --> 0:44:01,560
 to actually think for themselves.

488
0:44:01,560 --> 0:44:08,920
 We can use technology in a good way, but the proposal from the commission is not the way

489
0:44:08,920 --> 0:44:10,720
 to use technology.

490
0:44:10,720 --> 0:44:17,520
 We're trying to hide away things, we're trying to filter things, we're breaking encryption.

491
0:44:17,600 --> 0:44:25,360
 And instead of using technology to actually empower kids to be able to identify danger

492
0:44:25,360 --> 0:44:31,960
 and seek help when they need it, and also to have the privacy that they also deserve.

493
0:44:31,960 --> 0:44:37,440
 So we need to actually use technology to empower children and to help them, rather than to

494
0:44:37,440 --> 0:44:40,480
 take away their freedom and their rights.

495
0:44:40,480 --> 0:44:46,240
 We have a couple of minutes left, one or two, for questions, and I don't think we have like

496
0:44:46,280 --> 0:44:51,200
 a thank you slide, we're not very polite, we're Danish.

497
0:44:51,200 --> 0:44:55,560
 So I think there is a roaming mic if you have a question or two, and otherwise we'll go

498
0:44:55,560 --> 0:44:58,560
 out of the tent and we'll be happy to take questions.

499
0:45:09,160 --> 0:45:14,840
 You started out by saying that your way into this was going to tell these people that we're

500
0:45:14,920 --> 0:45:22,680
 also trying to protect the children, but you didn't really come up with very much in terms

501
0:45:22,680 --> 0:45:25,720
 of alternatives.

502
0:45:25,720 --> 0:45:27,760
 What are the alternatives?

503
0:45:27,760 --> 0:45:35,040
 The alternatives are actually spending money on having police enforcement looking at investigations

504
0:45:35,040 --> 0:45:41,720
 and looking at the criminal networks that are distributing and creating child sexual

505
0:45:41,720 --> 0:45:43,240
 abuse material.

506
0:45:43,240 --> 0:45:51,680
 It is also having more responsible social media platforms, for example, and also having

507
0:45:51,680 --> 0:45:57,480
 like use of, for example, pop-up functions if they detect you're being asked to share

508
0:45:57,480 --> 0:46:04,680
 your address or meet with somebody online, that you have like a pop-up that's saying,

509
0:46:04,680 --> 0:46:08,720
 well, do you know this person?

510
0:46:08,720 --> 0:46:12,400
 Is this somebody that you trust that you want to meet with?

511
0:46:12,560 --> 0:46:15,640
 Do you really want to share your address with the person?

512
0:46:15,640 --> 0:46:19,480
 Do you want to share your phone number with them?

513
0:46:19,480 --> 0:46:25,360
 Help kids also to, if they want to share nude images, to think about it before they do so

514
0:46:25,360 --> 0:46:27,600
 they make conscious choices.

515
0:46:27,600 --> 0:46:32,600
 And also to make sure that we don't spend a lot of money on technology that's not going

516
0:46:32,600 --> 0:46:40,120
 to be working, but actually spend it on supporting kids and the children's rights organizations

517
0:46:40,200 --> 0:46:42,600
 rather than just spending it on technology.

518
0:46:42,600 --> 0:46:46,920
 But it's not been written up as a formal proposal or amendment?

519
0:46:46,920 --> 0:46:54,800
 I have written it up as formal amendments that were then taken out of the Equality Committee.

520
0:46:54,800 --> 0:47:00,200
 The problem as well is that we have, for example, Edry that's saying we should take the proposal

521
0:47:00,200 --> 0:47:06,160
 completely off the table and say it's so bad that we shouldn't even be discussing it and

522
0:47:06,200 --> 0:47:11,320
 therefore don't want to engage with the proposal and trying to see how we can improve it.

523
0:47:11,320 --> 0:47:13,880
 And then we have the people that just love it.

524
0:47:13,880 --> 0:47:18,520
 And I've been trying to strike a middle ground saying, if this is going to go through, we

525
0:47:18,520 --> 0:47:26,400
 need to improve it a lot so that it's acceptable what's being put into law at the end.

526
0:47:26,400 --> 0:47:31,840
 But it's easier to say, let's keep out of it and ask the commission to take it off the

527
0:47:31,840 --> 0:47:33,640
 table completely.

528
0:47:33,640 --> 0:47:39,560
 And that's why there's not a lot of us trying to engage with the proposal in terms of trying

529
0:47:39,560 --> 0:47:43,840
 to improve it rather than just getting it off the table or saying that it's wonderful.

530
0:47:43,840 --> 0:47:48,280
 I think it's also the case that the commission spent typically years preparing a proposal,

531
0:47:48,280 --> 0:47:53,200
 doing multiple rounds of consultation, writing hundreds of pages of impact assessments, considering

532
0:47:53,200 --> 0:47:54,960
 policy options.

533
0:47:54,960 --> 0:48:00,880
 And for a civil society organization like Edry to sort of do that, well, we've thought

534
0:48:00,920 --> 0:48:06,280
 about it, but which I know we can't do that in a credible way.

535
0:48:06,280 --> 0:48:10,520
 So the right thing to do is call for the proposal to be rejected so that the commission, the

536
0:48:10,520 --> 0:48:16,600
 next commission, without Ylva Johansson, can come up with a better proposal to protect

537
0:48:16,600 --> 0:48:22,440
 prison online.

538
0:48:22,440 --> 0:48:26,360
 Maybe we should move it outside for, so yes, by all means.

539
0:48:26,360 --> 0:48:28,920
 Further questions is outside.

540
0:48:28,960 --> 0:48:31,160
 Thank you for being here, Karen and Jesper.

541
0:48:31,160 --> 0:48:32,920
 Let's give them a round of applause.